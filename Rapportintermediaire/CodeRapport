% il existe plusieures classes de documents
% pour des documents plus longs, vous pouvez utiliser
% book ou report
\documentclass[a4paper]{article} 


\usepackage{graphicx}

\usepackage{xcolor}

\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\small,       % Police monospace, taille petite
    keywordstyle=\color{blue},        % Mots-clés en bleu
    commentstyle=\color{gray},        % Commentaires en gris
    stringstyle=\color{red},          % Chaînes de caractères en rouge
    numbers=left,                     % Numéros de ligne à gauche
    numberstyle=\tiny,                % Style des numéros de ligne
    stepnumber=1,                    % Numéroter chaque ligne
    numbersep=5pt,                   % Espace entre numéros et code
    breaklines=true,                 % Coupe les lignes trop longues
    frame=single,                    % Cadre autour du code
    tabsize=2,                      % Taille de la tabulation (2 espaces)
    captionpos=b,                   % Position de la légende en bas
    showspaces=false,               % Ne pas montrer les espaces
    showstringspaces=false          % Ne pas montrer les espaces dans les chaînes
}

% vous pouvez changer les paramètres : voici les options dispoinibles :
% - a4paper
% - fancysections
% - notitlepage ou titlepage
% - onside ou twoside selon si vous voulez l'imprimer en recto-verso ou en recto
% - sectionmark
% - chaptermark (pour les 
% - pagenumber
% - enmanquedinspiration
% en cas de doutes, pas de doutes, la documentation est sur :
%  https://gitlab.binets.fr/typographix/polytechnique/-/blob/master/source/polytechnique.pdf
\usepackage[a4paper,  fancysections,  titlepage]{polytechnique}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{blindtext}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath, amssymb}


% nous avons défini deux commandes :
\newcommand{\code}[1]{%
    \mbox{\ttfamily%
        \detokenize{#1}%
    }%
}

\newcommand{\resultat}[1]{%
    \quad \rightsquigarrow \quad #1%
}

\author{Louis Vaneau}
\date{\today}
\title{Grand titre}
\subtitle{Petit titre}%
% pour changer de logo, ajoutez l'image dans un fomat PDF
% ou image en la glissant à droite et remplacez typographix
% par le nom de l'image, si vous ne voulez pas de logo, 
% supprimze la ligne. 
\logo{typographix}




\begin{document}

	\maketitle 
	
	\tableofcontents
	
	\section{Motivations}
		\subsection{Motivations générales}
Depuis son développement au XXe
siècle, la physique quantique a permis une avancée technologique
spectaculaire lors de la première révolution quantique. Une grande variété de systèmes physiques a ainsi
pu être étudiée : circuits supraconducteurs, objets quantiques individuels (atomes, électrons. . .), ou encore
défauts d’un cristal périodique.
Cependant, la découverte et la théorisation de l’intrication quantique ont ouvert la voie à de nouvelles
recherches : c’est ce que l’on appelle la seconde révolution quantique. Celles-ci visent à développer de
nouvelles technologies exploitant les principes de superposition et d’intrication. Ce domaine repose sur
quatre piliers principaux : la communication (cryptologie quantique), les ordinateurs quantiques, les
capteurs et les simulateurs quantiques. Les grandes puissances scientifiques déploient ainsi d’ambitieux
projets contribuant à cette seconde révolution (par exemple le Plan Quantique en France).\newline
Dans tous ces travaux, il est crucial de diagonaliser l’Hamiltonien du système afin de déterminer ses
énergies et états propres, et en particulier l’état fondamental. Il faut donc quantifier, représenter et
calculer les grandeurs physiques d’intérêt de manière fiable et rapide. Or, la complexité des algorithmes
classiques croît exponentiellement avec la taille des systèmes, les rendant inapplicables en pratique.
En considérant un système de spins, on constate que la dimension de l’espace de Hilbert double à
chaque particule ajoutée. Par exemple, le plus grand supercalculateur actuel, Frontier, dispose de 9 PB (9×1015
octets) de mémoire. Chaque nombre complexe nécessitant 16 octets pour être stocké, Frontier peut contenir
environ 5 × 1014 nombres complexes, soit un vecteur d’état complet pour seulement 49 particules de spin 1/2.
De plus, cette capacité ne permet que le stockage, et non les calculs associés.
Ainsi, afin de tirer parti des opportunités offertes par la seconde révolution quantique, il devient nécessaire de développer de nouvelles méthodes permettant de surmonter l’explosion combinatoire de l’espace
des états.
	
	
	\subsection{Applications de cette étude}
    
    Les méthodes variationnelles développées dans ce travail, combinant Monte Carlo variationnel et ansatz neuronaux tels que CNN et RBM, fournissent des références précises pour des systèmes de spins fortement corrélés et faiblement désordonnés. Ces résultats ont deux applications principales en physique quantique contemporaine.
\subsection*{Les matériaux supraconducteurs}

    Premièrement, ces avancées computationnelles contribuent directement à la compréhension des supraconducteurs à haute température critique (HTc), tels que les cuprates ou les fermions lourds, où la supraconductivité émerge dans un régime de corrélations électroniques très fortes. Dans ces matériaux, les électrons ne peuvent plus être décrits comme des quasi-particules faiblement interactives : leur comportement collectif domine.
    Le calcul précis de l’énergie fondamentale et de la structure de l’état fondamental est alors essentiel
    \begin{itemize}
    \item pour identifier les phases magnétiques ou paramagnétiques concurrentes, 
    \item comprendre les mécanismes de couplage
    \item analyser l’influence du désordre — qu’il s’agisse de dopage, d’impuretés ou de défauts cristallins — omniprésent dans les matériaux réels.
\end{itemize}



Cependant, ces modèles deviennent rapidement inaccessibles aux méthodes exactes dès que la taille du système ou le spin effectif augmente, en raison de la croissance exponentielle de l’espace de Hilbert. C’est précisément ici que les méthodes variationnelles modernes, combinant Monte Carlo variationnel et ansatz neuronaux (CNN, RBM), prennent toute leur importance. Elles permettent de réaliser des calculs fiables d’énergies fondamentales et de corrélations pour des systèmes de grande taille avec désordre, fournissant ainsi des outils puissants pour mieux cerner les mécanismes microscopiques complexes à l’origine de la supraconductivité à haute température.
	
	\subsection*{Ordinateur quantique}
    Ensuite, les résultats obtenus par cette étude constituent des références cruciales pour le "benchmarking" (étalonnage, test de performance) des plateformes quantiques en cours de développement. Avec l’essor des ordinateurs quantiques dits NISQ (noisy intermediate-scale quantum, c'est-à-dire des ordinateurs quantiques de taille intermédiaire et sujets au bruit), il est essentiel de valider leur capacité à simuler des systèmes quantiques complexes. Nos simulations classiques précises de modèles de spins larges et désordonnés offrent des bancs d’essai rigoureux permettant d’évaluer la fidélité des processeurs quantiques à reproduire les propriétés fondamentales de ces systèmes. Ce benchmarking est un levier indispensable pour améliorer la cohérence des qubits, la fidélité des portes logiques et les techniques de correction d’erreur, assurant un lien étroit entre théorie et expérimentation en informatique quantique.


Enfin, cette étude éclaire le développement algorithmique autour du Variational Quantum Eigensolver (VQE), un algorithme hybride classique–quantique visant à approximer l’état fondamental de systèmes quantiques. Ces algorithmes utilisant les mêmes principes que ceux décrits dans cette étude sont en cours de développement. L’analyse classique des ansatz neuronaux apporte ainsi des connaissances sur l’efficacité des circuits variationnels susceptibles d’être implémentés sur des dispositifs quantiques, contribuant ainsi à concevoir des protocoles VQE plus performants.
	
	\subsection{Faire des énumérations}
	Voici une liste :
	\begin{itemize}
		\item un premier élément ;
		\item un second élément.
	\end{itemize}
	
	On peut faire des liste numérotée :
	\begin{enumerate}
		\item de 1 ;
		\item de 2.
	\end{enumerate}
	
	\begin{description}
		\item [Éléphant : ] gros mammifère.
        \item [Moineau : ] petit oiseau.
	\end{description}

    \section{Des majuscules}
Notre PSC explore une approche prometteuse pour pallier ces limitations de stockage et de calcul dans des systèmes quantiques complexes (systèmes de $N$ spins, atomes de Rydberg\dots). Cette approche repose sur l’utilisation de l’apprentissage machine appliqué aux problèmes de physique quantique.

Cette méthode s’appuie sur trois piliers~:
\begin{itemize}
    \item la paramétrisation de l’espace de Hilbert de dimension $2^N$ par un espace réduit, via une compression non linéaire. Les états ainsi représentés sont appelés \emph{états quantiques neuronaux} (NQS) ;
    \item l’approximation de grandeurs physiques grâce à l’utilisation de méthodes statistiques, comme la méthode de Monte Carlo variationnelle (VMC), pour calculer efficacement ces grandeurs. 
    \item  L'exploration de l’espace paramétré des NQS grâce à l'optimisation et l'apprentissage via des fonctions de perte et leurs gradients.
\end{itemize}
\subsection*{Compression de l'état quantique}
Tout d'abord, afin de régler le problème de la croissance exponentielle de l'espace des états, nous allons compresser l'information. 
Soit W l'espace complexe des paramètres, on définit alors la fonction :
\[
\Psi :
\begin{array}{rcl}
W & \rightarrow & \mathcal{H}, \\
\theta & \mapsto & |\psi_\theta\rangle
\end{array}
\]


On nomme ces fonctions les \emph{ansatz variationnels} (ou ansatz).


Si 
\[
\dim[W] \sim \text{Poly}(N) \ll \dim[\mathcal{H}]
\]


alors la complexité n'est plus que polynomiale et le problème de complexité est réglée. 

D'autre part, nous savons qu'en dimension finie, notre vecteur d'état peut être décomposée de manière unique sur la base des états qui sont dans notre système de spins $2^N$. 
La fonction ci-dessus est donc bijective. (Les $xi$ représentant les spins soit en haut ou en bas).

\[
\begin{array}{rcl}
B_\mathcal{H} & \rightarrow & \mathbb{C} \\
|x_1, \dots, x_N\rangle & \mapsto & \langle x_1, \dots, x_N|\psi\rangle := \psi_{x_1, \dots, x_N}
\end{array}
\]

A partir de $\theta$, pour pouvoir utiliser efficacement les ansatz, on doit pouvoir connaître la décomposition de l'état $|\psi_\theta\rangle$ dans la base.

C'est pourquoi les ansatz sont généralement donnés sous la forme : 

\[
\Psi_\theta(x_1, \dots, x_N) = \langle x_1, \dots, x_N | \psi_\theta \rangle
\]

A chaque $\theta$ est associé un nouvel état car c'est alors une nouvelle décomposition dans la base des états de notre espace de Hilbert.

\subsection*{Monte-Carlo Variationnel}
Notre but principal est de calculer l'énergie fondamentale. Nous devons donc trouver un moyen de calculer cette énergie seulement avec les ansatz variationnels dépendants de $|\psi_\theta \rangle$.


Or, on peut montrer, en décomposant l'état $|\psi_\theta \rangle$ dans la base des états, qu'on a :

\[
\langle E \rangle_\theta
= \frac{\langle \psi_\theta | \hat{H} | \psi_\theta \rangle}{\langle \psi_\theta | \psi_\theta \rangle}
= \sum_{x} \frac{|\psi_\theta(x)|^2}{\langle \psi_\theta | \psi_\theta \rangle} \frac{\langle x | \hat{H} | \psi_\theta \rangle}{\langle x | \psi_\theta \rangle}
= \sum_{x} p_\theta(x) H_{\text{loc}}^\theta(x)
= \mathbb{E}_{p_\theta(x)} \left[ H_{\text{loc}}^\theta(x) \right]
\]



Avec \[
H_{\text{loc}}^\theta(x) = \frac{\langle x | \hat{H} | \psi_\theta \rangle}{\langle x | \psi_\theta \rangle}
\]
\[
p_\theta(x) = \frac{|\psi_\theta(x)|^2}{\langle \psi_\theta | \psi_\theta \rangle}
\]
et \[
\psi_\theta(x) = \langle x | \psi_\theta \rangle
\]

Ainsi $\psi_\theta(x)$ représente physiquement la probabilité associée à l'état $x$ pour le paramèttre $\theta$.

Malheureusement, nous nous retrouvons face au même problème qu'initialement car nous ne pouvons pas calculer $|\psi_\theta(x)|^2$ et donc $\psi_\theta(x)$ pour tout élément de la base $x$ à cause de la dimension exponentielle de l'espace de Hilbert.

Pour cela, on va utiliser les méthodes d'approximation de Monte-Carlo, 
\[
\langle E \rangle_\theta = \mathbb{E}_{p_\theta(x)} \left[ H_{\text{loc}}^\theta(x) \right] \approx \frac{1}{N_s} \sum_{x \in \{x\}} H_{\text{loc}}^\theta(x)
\]

où $N_s$ ets le nombre d'échantillons et où $x$ suit la loi $p_\theta(x)$.

Si la variance de l'estimateur croît polynomialement avec la taille du système
\[
\mathrm{Var}\!\left[ H_{\mathrm{loc}} \right] \sim \mathrm{Poly}(N),
\]
alors un nombre polynomial d'échantillons suffira pour toutes les tailles de système,
et nous disposerons de méthodes de calcul dont le coût évolue de manière polynomiale.
La combinaison de la compression qui résout le problème de stockage et l'utilisation des méthodes Monte-carlo qui permet de diminuer la complexité de calcul se nomme le Monte-Carlo Variationnel.


\subsection*{Descente de gradient}

Afin de calculer la moyenne de l'énergie dans un état donné grâce à l'approximation de Monte-Carlo, nous devons échantilloner des variables aléatoires $x$ selon la loi $p_\theta(x)$. 

Nous utilisons alors des chaînes de Markov homogènes et l'algorithme de Metropolis-Hastings. Cet algorithme nous permet de construire une chaîne de Markov qui possède une mesure invariante pré-déterminée donc $p_\theta(x)$ dans notre cas.

En notant $P(x,y)$ la matrice de transition de cette chaîne de Markov et $Q(x,y)$ une matrice symétrique positive au choix, on a :

\[
P(x,y) = Q(x,y)\,\min\!\left( \frac{\lvert \phi_{\theta}(y) \rvert^2}{\lvert \phi_{\theta}(x) \rvert^2},\, 1 \right)
\]

En ajoutant pour chaque état $x$ de la chaîne de Markov, le terme $H_{\text{loc}}^\theta(x)$ à la somme définissant $\langle E \rangle_\theta$, on aura, (même si les premiers états ne seront pas distribués selon $p_\theta(x)$) convergence vers $\langle E \rangle_\theta$.

En mécanique quantique, nous savons que \[
\forall\, \theta,\quad E_{\theta} \ge E_{0}
\]

Notre but est alors d'atteindre des valeurs d'énergie toujours plus basses car nous savons qu'elles ne seront que des bornes supérieures de l'énergie de l'état fondamental.

Nous devons donc réaliser une \textbf{Descente de gradient}
pour trouver le minimum global de la courbe $E$ en fonction de $\theta$. Nous aurons alors notre état fondamental, c'est-à-dire  $|\psi_\theta \rangle$ avec le $\theta$ en question.



A partir de l'itération $n$, nous savons alors quelle doit être la valeur de  $\theta$ pour progresser à l'étape $n+1$.

\[
\theta^{(n+1)} = \theta^{(n)} - \eta \,\nabla_{\theta^\ast} E(\theta)
\]
où $\eta$ est notre pas d'apprentissage.

De nouveau, en décomposant dans notre base d'états, il est possible de montrer que 

\[
\partial_{\theta_k^{\ast}} E
=
\mathbb{E}_{\sigma \sim |\psi(\sigma)|^2}
\left[
\left( \partial_{\theta_k} \log \psi(\sigma) \right)^{\ast}
\left(
H_{\mathrm{loc}}(\sigma) - \langle E \rangle
\right)
\right]
\]

On réecrit cela comme :

\[
\partial_{\theta_k^{\ast}} E
=
\frac{1}{N_s} J^{\dagger} \mathbf{E}_{\mathrm{loc}}
\]
avec \[
J_{ij} = \partial_{\theta_j} \log \psi_{\theta}(x_i)
\]
la matrice jacobienne et le vecteur d'énergies locales recentrées (de dimension $N_s$ le nombre d'échantillons)
\[
\mathbf{E}_\mathrm{loc}
=
H_{\mathrm{loc}}(\sigma) - \langle E \rangle
\]

On peut également montrer que \[
V_{\theta} \bigl( H_{\mathrm{loc}}(x_i) \bigr) = V(\hat{H})
\]
 Cette formule est intéressante car elle nous dit qu'à l'état fondamentale comme $V(\hat{H})$ est nulle, $V_{\theta} \bigl( H_{\mathrm{loc}}(x_i) \bigr)$ le sera donc notre estimation sera très précise. 

\begin{lstlisting}[language=Python, caption=Code de la descente de gradient-Code "jouet"]

def optimize_SGD(model, sampler, ham, chain_length, n_iters=1000, learning_rate=1e-2):
    # Initialize
    parameters = model.init(jax.random.key(1), jnp.ones((ham.hilbert.size,)))
    sampler_state = sampler.init_state(model, parameters, seed=1)

    # Logging
    logger = nk.logging.RuntimeLog()

    for i in tqdm(range(n_iters)):
        # sample
        sampler_state = sampler.reset(model, parameters, state=sampler_state)
        samples, sampler_state = sampler.sample(
            model, parameters, state=sampler_state, chain_length=chain_length
        )

        # compute energy and gradient
        E, E_grad = estimate_energy_and_gradient(model, parameters, ham, samples)

        # update parameters
        parameters = jax.tree.map(
            lambda x, y: x - learning_rate * y, parameters, E_grad
        )

        # log energy
        logger(step=i, item={"Energy": E})
    return logger, model, parameters, sampler
\end{lstlisting}

Nous avons écrit ce code afin de nous familiariser avec la syntaxe de Netket et pour ne pas utiliser de suite les options déjà toutes prêtes de cette bibliothèque pour la descente de gradient.

Remarques sur le code :

\begin{itemize}

\item ham est l'hamiltonien du système
\item model est ici le modèle qu'on a choisi pour notre ansatz, les paramètres de ce modèle seront le vecteur $\theta$
\item Sampler est un échantilloneur déjà présent dans NetKet, qu'on peut choisir. Nous utilisons le sampler de Monte-Carlo Hastings qui à partir d'un modèle et d'un état calcule la matrice de transition de la chaîne de Markov pour que la probabilité invariante soit \[
p_\theta(x) = \frac{|\psi_\theta(x)|^2}{\langle \psi_\theta | \psi_\theta \rangle}
\]
Les lignes 18 et 21 permettent d'estimer l'énergie moyenne de l'état et le gradient en ce point puis de modifier les paramètres comme souhaité. 
Finalement, le logger lignes 8 et 26 permet d'enregistrer à chaque itération la valeur de l'énergie.
\end{itemize}

Avant d'appliquer cette descente de gradient, nous devons soigneusement choisir l'ansatz car ils comportent tous des hypothèses sur les états et ne peuvent donc pas décrire le même ensemble d'états. 

On veut donc trouver l'ansatz qui permet de décrire le plus grand ensemble d'états et qui s'approche autant que possible de l'état du fondamental.

\subsection*{Ansatz variationnels}
\subsubsection*{Jastrow}
Tout d'abord, le premier ansatz que nous avons manipulé est celui de Jastrow : 
\[
\phi_J =\exp\left( X^\top J X \right)= \exp\left( \sum_{i=1}^n \sum_{j=1}^n X_i\, J_{ij}\, X_j \right)
\]
\(X \in \mathbb{R}^n\) est un vecteur colonne qui représente les spins haut ou bas et \(J \in \mathbb{R}^{n \times n}\) est une matrice symétrique. 
Cette fonction nous donne la valeur du coefficient du vecteur de la base \(X \in \mathbb{R}^n\) pour un J fixé

Le paramètre $\theta$ est donc ici la matrice J.

Physiquement, cet ansatz est symétrique par échange de deux particules. Les termes non diagonaux $Jij$ reflètent les interactions entre spins.

\begin{lstlisting}
class Jastrow(nn.Module):
    @nn.compact
    def __call__(self, x):
        n_sites = x.shape[-1]
        J = self.param("J", nn.initializers.normal(), (n_sites, n_sites), float)
        dtype = jax.numpy.promote_types(J.dtype, x.dtype)
        J = J.astype(dtype)
        x = x.astype(dtype)
        J_symm = J.T + J
        return jnp.einsum("...i,ij,...j", x, J_symm, x)
\end{lstlisting}
Remarques sur le code :
\begin{itemize}
    \item la ligne 5 crée le paramètre J et initialise ses coefficients avec une distribution normale. 
    \item la ligne 9 symmétrise la matrice J.
    \item la ligne 10 fait une somme selon la convention d'Einstein en indiquant les indices des différents termes.
\end{itemize}

Enfin, nous pouvons observer que les interactions ne s'effectuent que 2 par 2 car les termes sont en $XiJijXj$ liant le spin sur le site i et le spin sur le site j. Nous pouvons donc encore améliorer notre ansatz pour qu'il représente mieux les interactions entre spins.

\subsubsection*{Machine de Boltzmann}

Ensuite, afin de dépasser les limitations de Jastrow qui ne peut pas représenter une interaction à plus de 2 particules et donc inutilisable pour un mouvement collectif des spins, nous avons utilisé une machine de Boltzmann. 

\[
\log \Psi_{\mathrm{RBM}}(X)
=
\sum_k
\log \cosh\left(
b_k + \sum_i W_{ik} x_i
\right)
+ \sum_i a_i x_i
\]

 avec $b_k$ composante du vecteur b et $W$ une matrice comme pour Jastrow.

La machine de Boltzmann généralise l'ansatz de Jastrow en n'imposant pas que les interactions s'effectuent 2 par 2. 
En effet, en passant au log et en développant le cosh selon un développement limité nous avons : 

\[
\log \cosh(y) = \frac{1}{2} y^2 - \frac{1}{12} y^4 + o(y^4)
\]
Le premier terme en $y^2$ nous permet de retrouver Jastrow avec des interactions 2 à 2 mais tous les autres termes permettent d'envisager des interactions plus complexes à plus de 2 corps. 
Une machine de Boltzmann est en réalité un réseau de neurones avec une couche visible et une couche cachée.

METTRE IMAGE

Le vecteur d'entrée de données(qui représente les spins de notre système physique) $X$ subit un produit matriciel par la matrice $W$ avec un biais visible. Après être passé par une fonction d'activation, un biais visible (que l'image ne représente pas) est alors ajouté à l'input.

Nous avons tout d'abord codé une première version sans biais visible. 
\begin{lstlisting}
    class BM(nn.Module):
    alpha: float

    @nn.compact
    def __call__(self, x):
        n_sites = x.shape[-1]
        n_hidden = int(self.alpha * n_sites)

        W = self.param(
            "W", nn.initializers.normal(), (n_sites, n_hidden), jnp.complex128
        )
        b = self.param("b", nn.initializers.normal(), (n_hidden,), jnp.complex128)

        return jnp.sum(jnp.log(jnp.cosh(jnp.matmul(x, W) + b)), axis=-1)
\end{lstlisting}

Puis utilisant la méthode $Dense$ de Flax.linen, nous avons écrit un programme avec un biais visible où nous pouvions changer les paramètres (avec/sans couche visible, changer la fonction d'activation) plus facilement.

\begin{lstlisting}
    default_kernel_init = normal(stddev=0.01)


class RBM(nn.Module):

    param_dtype: Any = np.float64
    """The dtype of the weights."""
    activation: Any = nknn.log_cosh
    """The nonlinear activation function."""
    alpha: float | int = 1
    """feature density. Number of features equal to alpha * input.shape[-1]"""
    use_hidden_bias: bool = True
    """if True uses a bias in the dense layer (hidden layer bias)."""
    use_visible_bias: bool = True
    """if True adds a bias to the input not passed through the nonlinear layer."""
    precision: Any = None
    """numerical precision of the computation see :class:`jax.lax.Precision` for details."""

    kernel_init: NNInitFunc = default_kernel_init
    """Initializer for the Dense layer matrix."""
    hidden_bias_init: NNInitFunc = default_kernel_init
    """Initializer for the hidden bias."""
    visible_bias_init: NNInitFunc = default_kernel_init
    """Initializer for the visible bias."""

    @nn.compact
    def __call__(self, input):
        x = nn.Dense(
            name="Dense",
            features=int(self.alpha * input.shape[-1]),
            param_dtype=self.param_dtype,
            precision=self.precision,
            use_bias=self.use_hidden_bias,
            kernel_init=self.kernel_init,
            bias_init=self.hidden_bias_init,
        )(input)
        # features c'est le nombre d'unités cachées
        x = self.activation(x)
        # applique le x crée puis appliqué à input à la focntion d'activation ici le cosh
        x = jnp.sum(x, axis=-1)

        if self.use_visible_bias:
            v_bias = self.param(
                "visible_bias",
                self.visible_bias_init,
                (input.shape[-1],),
                self.param_dtype,
            )
            #cela crée le biais visible, avec une initialisation faite par "self.visible_biais_init"
            out_bias = jnp.dot(input, v_bias)
            #cela calcule le produit de input.transpose() par le v_bias
            # c'est le biais visible qu'on ajoute alors à la sortie de l'input ici x
            return x + out_bias
        else:
            return x
\end{lstlisting}
Remarques sur le code:
\begin{itemize}
    \item à la ligne 29, un objet $Dense$ est crée et cela fait un produit matricel et ajoute un biais caché si on le souhaite.
    \item un hyperparamètre (paramètre fixé à l'avance) important est $alpha$ car cela indique le rapport entre le nombre de neurones de la couche visible et le nombre de neurones de la couche cachée
    \item la fonction d'activation est cosh comme dans l'expression littérale ci-dessus mais d'autres fonctions d'activation peuvent être prises.
    \item enfin, à partir de la ligne 42, nous ajoutons un biais visible à l'input "transformé" par le réseau.
\end{itemize}
Le paramètre $\theta$ que nous optimisons au cours de la descente de gradient est ici les poids des liaisons entre les neurones contenus dans la matrice W ainsi que les biais visibles et cachées contenus dans les vecteurs $a$ et $b$.


\subsubsection*{Réseau de neurones convolutifs (CNN)}
Après la prise en main de la machine de Boltzmann, nous avons décidé d'utiliser une ansatz plus complète. En effet, bien que la machine de boltzmann puisse représenter des interactions plus complexes que Jastrow, comme elle n'a que deux couches de neurones, elle ne peut pas représenter la complexité des interactions entre spins.
En augmentant le nombre de neurones et le nombre de couches, nous pouvons espérer que plus on avance dans les couches du réseau plus l'information sera subtile et donc puisse saisir une grande partie de la physique du problème. 
Alors que la machine de Boltzmann envisage toutes les interactions entre spins, la CNN tire parti du fait que les interactions sont essentiellement locales mais elle va étudier avec les différents kernels des schémas d'interaction variés et complexes au fil des couches.
C'est pour cela que nous avons décidé d'utiliser ces réseaux de neurones convolutifs.

 Leur principe repose sur l’utilisation de filtres locaux appelés « kernels », qui parcourent systématiquement les données d’entrée (par exemple une configuration d’état) pour détecter des motifs caractéristiques. Chaque kernel est une petite matrice de poids (dont on peut choisir la taille), qui agit comme un détecteur de motifs locaux : il met en évidence certaines structures physiques, telles que des corrélations ou motifs d’interaction entre sites voisins.
 Lorsqu’un kernel est appliqué à l’ensemble de la donnée, il produit une nouvelle représentation appelée feature map (carte de caractéristiques), qui correspond à la présence locale du motif détecté à différentes positions.
 
 Par exemple si on a un système de spins 4*4, notre vecteur d'entrée sera un vecteur de 16 composantes une pour chaque spin. Il est préférable de le voir comme une matrice 4*4Si nous choisissons d'utiliser un kernel de taille 3*3, nous allons d'abord partir de la "gauche" de la matrice calculé le produit composante par composante avec le kernel pour obtenir un réel qui sera le coefficient de la feature map correspondant à la position du spin qui était au centre du kernel. Nous allons continuer ceci en se déplaçant dans la matrice avec un pas que l'on peut prendre de 1.
 
 Un CNN comporte généralement plusieurs kernels distincts, chacun produisant sa propre feature map, ce qui permet au réseau d’apprendre plusieurs types de motifs ou corrélations. Ces kernels sont organisés en channels : chaque channel correspond à l'ensemble des kernels qui s'appliquent sur une entrée. Plus on a de channels et de kernels dans chaque channels, plus le réseau peut capter des informations complexes.

Ensuite, ces cartes de caractéristiques peuvent être combinées et traitées par plusieurs couches convolutionnelles successives, permettant au réseau d’agréger l’information locale en représentations globales plus abstraites, utiles pour décrire l’état physique ou prédire ses propriétés. En résumé, la force des CNN réside dans leur capacité à apprendre des motifs locaux pertinents tout en exploitant la structure spatiale des données, ce qui les rend particulièrement adaptés pour modéliser des systèmes physiques avec des interactions locales ou des symétries spatiales. 

Nous avons alors codé la CNN :
\begin{lstlisting}
    class CNN(nn.Module):
    lattice: Lattice
    kernel_size: Sequence
    channels: tuple
    param_dtype: DType = complex

    def __post_init__(self):
        self.kernel_size = tuple(self.kernel_size)
        self.channels = tuple(self.channels)
        super().__post_init__()

    def setup(self):
        if isinstance(self.kernel_size[0], int):
            self.kernels = (self.kernel_size,) * len(self.channels)
        else:
            assert len(self.kernel_size) == len(self.channels)
            self.kernels = self.kernel_size

    @nn.compact
    def __call__(self, x):
        lattice_shape = tuple(self.lattice.extent)

        x = x / np.sqrt(2)
        _, ns = x.shape[:-1], x.shape[-1]
        x = x.reshape(-1, *lattice_shape, 1)
        for i, (c, k) in enumerate(zip(self.channels, self.kernels)):
            x = nn.Conv(
                features=c,
                kernel_size=k,
                padding="CIRCULAR",
                param_dtype=self.param_dtype,
                kernel_init=nn.initializers.xavier_normal(),
                use_bias=True,
            )(x)

            if i:
                x = logcosh_expanded_dv(x)
            else:
                x = logcosh_expanded(x)

        x = jnp.sum(x, axis=range(1, len(lattice_shape) + 1)) / np.sqrt(ns)
        x = nn.Dense(
            features=x.shape[-1], param_dtype=self.param_dtype, use_bias=False
        )(x)
        x = jnp.sum(x, axis=-1) / np.sqrt(x.shape[-1])
        return x
\end{lstlisting}
\begin{itemize}
    \item Ligne 13-18 initialisation des tailles des kernels et du nombre de channels, permettant de configurer la structure des couches convolutionnelles.
    \item Ligne 22-25 remodelage du vecteur d’entrée en tenseur 4D (batch de données, hauteur, largeur, canal) pour exploiter la structure spatiale de la grille.
    \item Ligne 26-37  application successive des convolutions avec padding (bordure ajoutée artificiellement pour les calculs avec les kernels afin que la sortie ait la même taille que l'entrée) circulaire, respectant la périodicité du réseau physique, suivie d’une activation non linéaire adaptée.
    \item Ligne 40-44 Couche dense finale sans biais qui combine les informations globales et calcule la sortie scalaire normalisée du réseau.
\end{itemize}


\subsection*{Momentum on en parle ?}

\subsection*{Descente de gradient naturel}
Diapositives de la présentation sur Whatsapp pour les différents paramèttres 

\subsection*{Prise en main de la syntaxe Netket }
Explique les 5 lignes de code qui font la descente de gradient (naturel)


\end{document}













